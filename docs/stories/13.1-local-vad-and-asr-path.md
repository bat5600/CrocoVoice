# Epic 13 — Local VAD/ASR Path

## Status
Ready for Review

## Story
**As a** privacy-focused user,
**I want** an optional local VAD/ASR path,
**so that** dictation can continue even when cloud services are unavailable.

## Acceptance Criteria
1. Local inference can be enabled when supported hardware is detected (or when a local model is present).
2. Local VAD/ASR can process short dictations end-to-end without cloud calls.
3. Local inference is gated by a feature flag and can be disabled safely.
4. Local inference output flows through the same dictionary/personalization pipeline as cloud output.
5. On Windows, onboarding offers optional local Whisper installation with presets **Lite / Balanced / Quality / Ultra** for FR+EN, with an automatic recommendation based on device capability.
6. Local model downloads are opt-in, resumable, checksum-verified, stored in app data, and can be removed/updated; no large model is shipped by default.
7. The selected local preset is persisted in settings and is reusable by other local transcription paths (e.g., long-form uploads) when available.

## Tasks / Subtasks
- [x] Integrate ONNX runtime or local ASR/VAD engine (AC: 1,2)
- [x] Add local model discovery + capability checks (AC: 1,3)
- [x] Wire local inference into the pipeline (AC: 2,4)
- [x] Add feature flag gating and safe disable behavior (AC: 3)
- [x] Define Whisper preset matrix (Lite/Balanced/Quality/Ultra) for FR+EN with capability thresholds (AC: 5,6)
- [x] Implement a local Model Manager (manifest, download, checksum, versioning, storage, cleanup) (AC: 6,7)
- [x] Add onboarding step for local model opt-in + download UX (AC: 5,6)
- [x] Add settings UI to manage models (install/upgrade/remove) and show active preset (AC: 6,7)
- [x] Wire preset selection into local ASR config and reuse for long-form local transcription when available (AC: 7)

## Dev Notes
- Keep this feature optional and gated by a feature flag.
- Avoid shipping large models unless explicitly configured.
- Target Windows first; use multilingual Whisper models (FR+EN), not `.en`-only variants.
- Preset “Ultra” should only be offered when hardware capacity is sufficient (CPU/GPU, RAM, disk).
- Onboarding should be skippable; downloads must be resumable and non-blocking.

## Preset Matrix (Draft — Windows, Whisper local)
**Goal:** simple 4-tier choice with automatic recommendation. Sizes depend on quantization; values below are rough ranges and should be finalized in the model manifest.

| Preset | Model (multilingual) | Intended hardware | Disk target | Notes |
| --- | --- | --- | --- | --- |
| Lite | tiny | CPU-only, ≥8 GB RAM | ~0.1–0.6 GB | Fastest, lowest accuracy; good for low-end devices |
| Balanced | base | CPU-only, ≥12 GB RAM | ~0.2–0.9 GB | Everyday balance for most laptops |
| Quality | small | CPU-only, ≥16 GB RAM | ~0.4–1.5 GB | Balanced speed/quality; default recommendation on most PCs |
| Ultra | medium (CPU) or large (GPU) | CPU ≥32 GB RAM **or** GPU ≥6 GB VRAM | ~1.5–6 GB | Best quality; CPU may be slow—prefer GPU |

**Recommendation logic (draft):**
- If compatible GPU detected → recommend Ultra (large).
- Else if RAM ≥16 GB and CPU supports AVX2 → recommend Quality (small).
- Else if RAM ≥8 GB → recommend Balanced (base).
- Else → recommend Lite (tiny).

### Testing
- Manual: enable local inference, verify dictation completes without cloud calls.
- Manual: onboarding opt-in installs a preset, verifies progress/resume, and local dictation works offline.
- Manual: uninstall/upgrade a model in settings, verify local path falls back safely.

## Change Log
| Date | Version | Description | Author |
| --- | --- | --- | --- |
| 2026-01-24 | v0.1 | New epic for local inference fallback | PO |
| 2026-01-26 | v0.2 | Extend scope: onboarding + model presets + local model manager | PO |
| 2026-02-01 | v0.3 | Local model manager + onboarding preset install + preset wiring | Dev |
| 2026-02-01 | v0.4 | Remplace le bouton d'import par un téléchargement preset | Dev |
| 2026-02-01 | v0.5 | CTA "Choisir un modèle" + statut téléchargé/poids estimé | Dev |
| 2026-02-01 | v0.6 | Choix preset déclenche download + toast progrès | Dev |
| 2026-02-01 | v0.7 | Choix preset auto-télécharge + toast progression persistante | Dev |
| 2026-02-01 | v0.8 | Ajout preset Balanced + manifest quantisé (Lite/Balanced/Quality/Ultra) | Dev |

## Dev Agent Record
### Agent Model Used
GPT-5 (Codex CLI)

### Debug Log References
N/A

### Completion Notes List
- Added optional local ASR command hook with safe fallback to cloud.
- Gated local inference via settings and feature flags.
- Added local Whisper preset matrix + model manager (download/resume/checksum/storage).
- Added onboarding step + settings UI to install/activate/remove local models.
- Wired local preset into command/server ASR paths (env + headers).
- Swapped settings CTA to “Télécharger un modèle” with preset selection.
- Swapped settings CTA to “Choisir un modèle” with download status + size hint.
- Choix du preset déclenche le téléchargement si besoin + toast de progression.
- Toasts de téléchargement avec barre de progression et états (annulé/erreur).
- Added Balanced preset + updated local model manifest to 4-tier selection.

### File List
- main.js
- preload.js
- dashboard.html
- dashboard.js

## QA Results
