# Epic 4 — History Metadata and Quality Metrics

## Status
Ready for Review

## Story
**As a** user,
**I want** history entries to retain raw/formatted/edited text and quality metrics,
**so that** I can understand output quality and track improvements over time.

## Acceptance Criteria
1. Each history entry stores `raw_text`, `formatted_text`, and `edited_text` (nullable if not produced).
2. Each entry stores quality metrics: `latency_ms`, `divergence_score`, `language`, and `mic_device`.
3. Metadata is persisted locally and remains available offline; if sync is enabled, metadata is included in the sync payload.
4. Metrics collection never blocks dictation or delivery, and failures fall back to safe defaults.
5. No sensitive content is uploaded without explicit user consent (metrics and metadata only).
6. Fallback path is recorded (streaming vs file upload vs local inference).

## Tasks / Subtasks
- [x] Extend the history table/schema to include text stages and metrics (AC: 1,2)
- [x] Capture metrics during the pipeline and persist with history entries (AC: 2,4)
- [x] Ensure sync payload includes metadata without blocking (AC: 3)
- [x] Guard metrics capture with privacy toggles (AC: 5)
- [x] Add fallback path field and populate on completion (AC: 6)

## Dev Notes
- `divergence_score` can be a simple edit distance ratio between raw and formatted text (v1).
- Keep defaults when metrics are unavailable (e.g., `latency_ms = null`).

### Testing
- Manual: verify history rows contain multiple text stages and metrics after dictation.
- Offline: ensure metadata persists and can be viewed without network.

## Change Log
| Date | Version | Description | Author |
| --- | --- | --- | --- |
| 2026-01-25 | v0.1 | New story to align PRD metrics requirements | PO |

## Dev Agent Record
### Agent Model Used
GPT-5

### Debug Log References
N/A

### Completion Notes List
- Champs multi-textes + métriques ajoutés à l’historique, avec divergence score.
- Capture latence/micro/fallback non bloquante et toggle de métriques.
- Sync étendu pour inclure les nouveaux champs.

### File List
- main.js
- store.js
- sync.js
- renderer.js
- dashboard.html
- dashboard.js

## QA Results

### Review Date: 2026-02-01

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
Review based on story documentation and file list (main.js, store.js, sync.js, renderer.js, dashboard.html, dashboard.js). Manual verification is noted; automated QA execution not recorded. History metadata and quality metrics storage and display.

### Refactoring Performed
None.

### Compliance Check
- Coding Standards: ✓ (per dev DoD; not re-verified in this pass; standards in docs/architecture/coding-standards.md)
- Project Structure: ✓ (file list confined to existing structure)
- Testing Strategy: ✗ (manual verification only; no automated coverage observed)
- All ACs Met: ✓ (per dev notes; not independently verified)

### Improvements Checklist

- [ ] Add tests for metric calculations and persistence across restarts.
- [ ] Add regression test for offline viewing of metadata.
- [ ] Verify metrics storage does not leak sensitive content.

### Security Review
Metrics may include derived data; ensure no unintended PII.

### Performance Considerations
Additional metadata writes may affect throughput; verify with larger histories.

### Files Modified During Review
None (QA artifacts only).

### Gate Status
Gate: CONCERNS -> docs/qa/gates/4.5-history-metadata-and-quality-metrics.yml
Risk profile: Not run
NFR assessment: Not run

### Recommended Status
[✗ Changes Required - See unchecked items above]
(Story owner decides final status)
